{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b99af9",
   "metadata": {},
   "source": [
    "Se aborda en esta primera parte del proyecto final *el problema de la determinaci√≥n del redshift*.\n",
    "\n",
    "Se muestra el mejor c√≥digo pensado para resolver este problema y que en cuya arquitectura sean puestos en pr√°ctica los diferentes temas abordados a lo largo del curso.\n",
    "\n",
    "Se dividir√° el c√≥digo en bloques con una tarea diferente, indicando su funci√≥n y sus implicaciones.\n",
    "\n",
    "Elaborado por: Oscar Ra√∫l S√°nchez Padilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3104eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOQUE 1: IMPORTACIONES Y CONFIGURACI√ìN\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn: Servir√° para separar los datos en train y test, har√° la validaci√≥n cruzada m√°s delante, Normalizar√° y/o escalar√° los datos y calcular√° m√©tricas.\n",
    "\n",
    "#Usaremos la siguiente librer√≠a para la cross-validation\n",
    "from sklearn.model_selection import train_test_split, KFold  \n",
    "\n",
    "#StandardScaler estandarizar√° las caracter√≠sticas de tus datos. Evitar√° darles pesos diferentes a datos de diferentes tama√±os o magnitudes.\n",
    "#RobustScaler nos funcionara como alternativa a StandardScaler que maneja mejor outliers(valores atipicos o fuera de lo normal)\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "#Las siguientes funciones servir√°n para evaluar qu√© tan bueno ser√° nuestro modelo de regresi√≥n comparando las predicciones con los valores reales. Son m√©tricas de rendimiento.\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# TensorFlow y Keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Las componentes principales para construir y entrenar redes neuronales:\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "\n",
    "# Usamos el siguiente algoritmo de optimizaci√≥n para entrenamiento de las redes neuronales:\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Para reproducibilidad fijamos los numeros \"aleatorios\" para que siempre sean iguales.\n",
    "# Esto es √∫til para saber si los cambios son por mi c√≥digo y no por azar. \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42) #Usamos el n√∫mero 42 como convenci√≥n dentro del mundo de la programaci√≥n, en general puede ser cualquier numero\n",
    "\n",
    "# Esto har√° que nuestras gr√°ficas se vean m√°s presentables (No afecta al c√≥digo en si mismo)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cacdf797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Cargando datos...\n",
      "   ‚úÖ CSV cargado: (492, 10)\n",
      "   ‚úÖ NPZ cargado: 492 espectros\n",
      "\n",
      "üìä Dataset construido:\n",
      "   - Espectros: (492, 7781)\n",
      "   - Dimensi√≥n espectral: 7781 p√≠xeles\n",
      "   - Redshift min/max: [-0.0011, 3.4567]\n",
      "   - Tipos espectrales √∫nicos: 3\n",
      "\n",
      "‚úÖ Normalizaci√≥n por espectro completada\n",
      "‚úÖ StandardScaler aplicado\n",
      "‚úÖ Shape para CNN: (492, 7781, 1)\n",
      "\n",
      "üîÄ Divisi√≥n train/test:\n",
      "   - Train: 393 espectros (79.9%)\n",
      "   - Test:  99 espectros (20.1%)\n",
      "\n",
      "‚úÖ Datos listos para entrenar\n"
     ]
    }
   ],
   "source": [
    "# BLOQUE 2: CARGA Y PREPROCESAMIENTO DE DATOS\n",
    "\n",
    "# Rutas de tus archivos (guardados localmente)\n",
    "csv_path = r'C:\\Users\\op354\\OneDrive\\Documentos\\Oscar\\UG\\Repositorio Github\\ejerciosytareas-Oscarraul28\\balanced_spectra_features.csv'\n",
    "npz_path = r'C:\\Users\\op354\\OneDrive\\Documentos\\Oscar\\UG\\Repositorio Github\\ejerciosytareas-Oscarraul28\\balanced_spectra_flux.npz'\n",
    "\n",
    "# Carga de datos\n",
    "print(\"üìÇ Cargando datos...\")\n",
    "df = pd.read_csv(csv_path) # Lee el archivo CSV y lo convierte en un DataFrame (tabla de pandas)\n",
    "npz = np.load(npz_path, allow_pickle=True) # Carga el archivo NPZ (formato comprimido de NumPy). Es como un diccionario que contiene arrays.\n",
    "targetids = npz['targetids'] # Extrae la lista de IDs de los objetos astron√≥micos\n",
    "flujo_arr = npz['flux_arrays'] # Extrae los espectros (arrays de flujo luminoso), donde cada espectro es un array de muchisimos pixeles\n",
    "\n",
    "print(f\"   ‚úÖ CSV cargado: {df.shape}\") # Aqu√≠ van los datos peque√±os\n",
    "print(f\"   ‚úÖ NPZ cargado: {len(flujo_arr)} espectros\") # Y aqu√≠ los datos grandes\n",
    "\n",
    "# Construir dataset alineando CSV con NPZ. Esto es muy importante porque...\n",
    "X_list = [] # Aqu√≠ se guardan los espectros (features)\n",
    "y_list = [] # Aqu√≠ se guardan los redshifts!! Que son los targets!!\n",
    "espectro_lista = [] #Aqu√≠ se guardan los tipos espectrales\n",
    "\n",
    "#Aqui hacemos el loop por cada objeto, donde terminamos obteniendo tanto el indice como su respectivo valor\n",
    "for i, tid in enumerate(targetids):\n",
    "    row = df.loc[df['targetid'] == tid] #Aqu√≠ busca la fila en el CSV que tenga ese targetid\n",
    "    if row.empty:\n",
    "        continue #Si no encuentra el objeto en el CSV salta al siguiente\n",
    "    redshift = row['redshift'].values[0] #Toma de la columna de redshift el primer (y unico) valor\n",
    "    if pd.isna(redshift):\n",
    "        continue #Aqu√≠ solo nos aseguramos de tener datos completos. Si el redshift es NaN, lo salta.\n",
    "    X_list.append(flujo_arr[i])\n",
    "    y_list.append(float(redshift))\n",
    "    espectro_lista.append(row['spectype'].values[0])\n",
    "\n",
    "# Guardamos todo:\n",
    "X = np.asarray(X_list)         # Espectros \n",
    "y = np.asarray(y_list)         # Redshift (target)\n",
    "spectypes = np.asarray(espectro_lista)  # Tipos espectrales (GALAXY, QSO, STAR,...)\n",
    "\n",
    "print(f\"\\nüìä Dataset construido:\")\n",
    "print(f\"   - Espectros: {X.shape}\") #Muestra la cantidad de espectros con la cantidad pixeles que contienen todos por igual\n",
    "print(f\"   - Dimensi√≥n espectral: {X.shape[1]} p√≠xeles\") # shape[1] nuestra el segundo n√∫mero = p√≠xeles por espectro\n",
    "print(f\"   - Redshift min/max: [{y.min():.4f}, {y.max():.4f}]\") # Rango de redshift en los datos mostrando 4 decimales\n",
    "print(f\"   - Tipos espectrales √∫nicos: {len(np.unique(spectypes))}\") # Encuentra valores √∫nicos y nos dice cuantos tipos diferentes hay\n",
    "\n",
    "# Normalizaci√≥n: dividir por m√°ximo absoluto de cada espectro\n",
    "eps = 1e-12 #Evita dividir por cero\n",
    "X_norm = X / (np.max(np.abs(X), axis=1, keepdims=True) + eps) # Tomamos el valor absoluto de todos los numeros para normalizar por la intensidad m√°xima, con axis=1 = operamos en cada fila (cada espectro) y keepdims=True = mantiene la dimensi√≥n.\n",
    "print(f\"\\n‚úÖ Normalizaci√≥n por espectro completada\")\n",
    "\n",
    "# Escalado global con StandardScaler\n",
    "scaler = StandardScaler() # Se crea el objeto escalador, fit calcula la media y desviaci√≥n est√°ndar de todos los datos y transform aplica la f√≥rmula: (X - media) / desv_std\n",
    "X_scaled = scaler.fit_transform(X_norm)\n",
    "print(f\"‚úÖ StandardScaler aplicado\")\n",
    "\n",
    "# Convertir a formato 3D para CNN (samples, timesteps, features)\n",
    "X_cnn = np.expand_dims(X_scaled, axis=-1) # Agrega una dimensi√≥n m√°s para en la tercer dimensi√≥n incluir el canal que nos da el color. 1 canal grises, 3 canales color.\n",
    "print(f\"‚úÖ Shape para CNN: {X_cnn.shape}\")\n",
    "\n",
    "# Divisi√≥n train/test estratificada por tipo espectral\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cnn, y, # Los datos a dividir: features (X) y target (y)\n",
    "    test_size=0.2, # 20% para test, 80% para train\n",
    "    random_state=42, # Semilla para reproducibilidad ya vista\n",
    "    stratify=spectypes # Organiza los tipos espectrales para train y test en la misma proporci√≥n\n",
    ")\n",
    "\n",
    "print(f\"\\nüîÄ Divisi√≥n train/test:\")\n",
    "print(f\"   - Train: {X_train.shape[0]} espectros ({100*len(X_train)/len(X):.1f}%)\")\n",
    "print(f\"   - Test:  {X_test.shape[0]} espectros ({100*len(X_test)/len(X):.1f}%)\")\n",
    "print(f\"\\n‚úÖ Datos listos para entrenar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
